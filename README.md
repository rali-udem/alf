RÉSUMÉ
ALF: Un jeu de données d’analogies françaises à grain fin pour l’évaluation de la connaissance lexicale des grands modèles de langue
La révolution indéniable apportée par les grands modèles de langue (LLM) provient de l’étonnante fluidité des textes qu’ils génèrent. Cette fluidité soulève une question scientifique essentielle : quelle quantité de connaissance lexicale les LLM capturent-ils réellement afin de produire un langage aussi fluide ? Pour y répondre, nous présentons ALF, un jeu de données analogiqes librement accessible et doté de riches informations lexicographiques fondées sur la théorie Sens-Texte. Il comprend 2600 analogies lexicales à grain fin avec lesquelles nous évaluons la capacité lexicale de quatre LLM standards : ChatGPT-4o mini, Llama3.0-8B, Llama3.1-8B et Qwen2.5-14B. En moyenne, ChatGPT et la série Llama obtiennent une précision aux environs de 55%, tandis que Qwen est juste en dessous du seuil des 60%, ce qui montre qu’ALF pose un défi considérable. Nous identifions en outre certains types d’analogies et de méthodes d’invite qui révèlent des disparités de performance.

ABSTRACT
ALF : A Fine-Grained French Analogy Dataset for Evaluating Lexical Knowledge of Large Language Models
The undeniable revolution brought forth by Large Language Models (LLMs) stems from the amazing fluency of the texts they generate. This fluency raises a key scientific question : how much lexical knowledge do LLMs actually capture in order to produce such fluent language ? To address this, we present ALF, a freely-available, analogy dataset endowed with rich lexicographic information grounded in Meaning-Text Theory. It comprises 2600 fine-grained lexical analogies with which we evaluate the lexical ability of four off-the-shelf LLMs, namely ChatGPT-4o mini, Llama3.0-8B, Llama3.1-8B, and Qwen2.5-14B. On average, ChatGPT and the Llama series perform at around 55% accuracy, whereas Qwen reaches just below the 60% threshold, thus qualifying ALF as a challenging dataset. We further identify certain types of analogies and prompting methods that reveal performance disparities.





